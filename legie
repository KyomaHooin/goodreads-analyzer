#!/usr/bin/python
# -*- coding: utf-8 -*-
#
# Legie Analyzer 1.1
#
# apt-get install python-requests python-matplotlib
#
# "https://www.legie.info/kniha/top/9999/zanr/sci-fi"
#

import requests,StringIO,lxml.html,argparse,numpy,json,time,sys,re

#--------------------------------------

LEGIE='https://www.legie.info/kniha/top/9999/zanr/sci-fi'

#--------------------------------------

parser = argparse.ArgumentParser(description='Goodreads Analyzer 1.1')
group = parser.add_mutually_exclusive_group(required=True)
group.add_argument('--scrap', help='Scrap HTML data.', nargs=1, metavar='JSON')
group.add_argument('--filter', help='Filter data.', nargs=1, metavar='JSON')
filter = parser.add_argument_group('Filter options')
filter.add_argument('--avg-min', help='Average rating min. [0-100]', type=float, default=0)
filter.add_argument('--avg-max', help='Average rating max. [0-100]', type=float, default=100)
filter.add_argument('--cnt-min', help='Avg rating count min.', type=int, default=0)
filter.add_argument('--cnt-max', help='Avg rating count max.', type=int, default=1000000000)
args = parser.parse_args()

#--------------------------------------

print('Legie Analyzer 1.0\n')

if args.scrap:
	
	# SCRAP & PARSE

	print('Scraping data ..')

	DATASET = {}

	session = requests.Session()
	req = session.get(LEGIE)

	if req.status_code == 200:
		p = lxml.html.HTMLParser()
		t = lxml.html.parse(StringIO.StringIO(req.text), p)
		o = t.xpath("//table[@class='tabulka-s-okraji']//tr")
		for i in range(0,len(o)):
			data = o[i].xpath(".//td")
			if len(data) == 5:
				book_id = data[0].text.replace('.','').strip()
				book_avg = data[3].text.replace('%','').strip()
				book_cnt = data[4].text.strip()
		
				if data[1].text:
					book_title = data[1].text.strip().encode('utf-8')
				else:
					book_title = data[1].xpath(".//a")[0].text
				if data[2].text:
					book_author = data[2].text.strip().encode('utf-8')
				elif data[2].xpath(".//a"):
					book_author = data[2].xpath(".//a")[0].text.strip().encode('utf-8')
				else:
					book_author = ''

				DATASET[str(book_id)] = {
					'title':book_title,
					'author':book_author,
					'avg':int(book_avg),
					'cnt':int(book_cnt)
				}
	print('Done.')

	# WRITE DATASET
	try:
		with open(args.scrap[0], 'w') as f: json.dump(DATASET, f)
	except:
		print('Failed to write dataset.')
		sys.exit(1)
	
if args.filter:
	# LOAD DATASET
	DATASET={}
	try:
		with open(args.filter[0], 'r') as f: DATASET = json.load(f)
	except:
		print('Failed to load dataset.')
		sys.exit(1)
	
	# FILTER
	for book in DATASET:
		if DATASET[book]['avg'] > args.avg_min and DATASET[book]['avg'] < args.avg_max:
			if DATASET[book]['cnt'] > args.cnt_min and DATASET[book]['cnt'] < args.cnt_max:
				print(
					str(DATASET[book]['avg']) + ' - ' +
					DATASET[book]['title'].encode('utf-8') + ' - ' +
					DATASET[book]['author'].encode('utf-8')
				)

# EXIT
sys.exit(0)

