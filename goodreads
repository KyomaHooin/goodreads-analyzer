#!/usr/bin/python
# -*- coding: utf-8 -*-
#
# Goodreads Analyzer 1.1
#
# apt-get install python-requests python-matplotlib
#
# https://www.goodreads.com/shelf/show/cyberpunk (p.1-25)
# https://www.goodreads.com/shelf/show/science-fiction (p.1-25)
# https://www.goodreads.com/list/show/19341.Best_Science_Fiction (p.1-29)
#

import sqlite3,requests,StringIO,lxml.html,argparse,numpy,json,time,sys,re
import matplotlib.pyplot as plt
from numpy import mean

#--------------------------------------

FIREFOX='/home/user/.mozilla/firefox/2wfpvx7b.default/cookies.sqlite'

GOOD='https://www.goodreads.com'

#SHELF = GOOD + '/shelf/show/cyberpunk'
#SHELF = GOOD + '/shelf/show/science-fiction'
#SHELF = GOOD + '/list/show/19341.Best_Science_Fiction'

LAST_PAGE = 29

OUTFILE='cyber-dataset.json'
#OUTFILE='scifi-dataset.json'
#OUTFILE='best-scifi-dataset.json'

OUTPNG='best-scifi-all.png'

PLOT_TITLE='Underrated Cyberpunk < 1990'

#--------------------------------------

parser = argparse.ArgumentParser(description='Goodreads Analyzer 1.1')
group = parser.add_mutually_exclusive_group(required=True)
group.add_argument('--scrap', help='Scrap HTML data.', action='store_true')
group.add_argument('--plot', help='Plot scrap from CSV file.', action='store_true')
group.add_argument('--filter', help='Filter CSV data.', action='store_true')
filter = parser.add_argument_group('Filter options')
filter.add_argument('--year-min', help='Minimum year.', type=int, default=0)
filter.add_argument('--year-max', help='Maximum year.', type=int, default=2048)
filter.add_argument('--avg-min', help='Average rating min.', type=float, default=0)
filter.add_argument('--avg-max', help='Average rating max.', type=float, default=5)
filter.add_argument('--cnt-min', help='Avg rating count min.', type=int, default=0)
filter.add_argument('--cnt-max', help='Avg rating count max.', type=int, default=1000000000)
args = parser.parse_args()

#--------------------------------------

print('Goodreads Analyzer 1.0\n')

if args.scrap:
	# COOKIE
	#
	# https://goodreads.com -> Remember login -> Login -> Quit

	COOKIES={}

	try:
		conn = sqlite3.connect(FIREFOX)
		cur = conn.cursor()
		cur.execute("SELECT name,value FROM moz_cookies WHERE host = 'www.goodreads.com'")
		rows = cur.fetchall()
		for name,value in rows: COOKIES[name] = value
	except:
		print('Failed to get cookies.')
		sys.exit(1)

	# SCRAP & PARSE

	print('Scraping data .. (this may take a while)\n')

	DATASET = {}

	session = requests.Session()

	for i in range(1, LAST_PAGE + 1):
		print('Page: ' + str(i))
		req = session.get(SHELF + '?page=' + str(i), cookies=COOKIES)
		if req.status_code == 200:
			p = lxml.html.HTMLParser()
			t = lxml.html.parse(StringIO.StringIO(req.text), p)
			o = t.xpath("//div[@class='elementList']")
			for i in range(0,len(o)):
				book = o[i].xpath(".//a[@class='bookTitle']")
				if book:
					book_title = book[0].text.encode('utf-8')
					book_url = book[0].get('href').encode('utf-8')
					book_id = re.sub('.*/(\d+).*', '\\1', book_url)
				author = o[i].xpath(".//a[@class='authorName']//span")
				if author:
					book_author = author[0].text.encode('utf-8')
				data = o[i].xpath(".//span[@class='greyText smallText']")
				if data:
					book_avg = re.findall('avg rating.*', data[0].text)
					book_ratings = re.findall('.*ratings', data[0].text)
					book_published = re.findall('published.*', data[0].text)
				DATASET[book_id] = {
					'title':re.sub('[(].*', '', book_title.replace(';',' -')).strip(),
					'author':book_author,
					'url':book_url,
					'avg':float(re.sub('avg rating (.*) .*', '\\1', book_avg[0])),
					'ratings':int(re.sub('(.*) ratings.*', '\\1', book_ratings[0]).replace(',','').strip()),
					'year':int(re.sub('published (.*)', '\\1', book_published[0]).strip())
				}

			# Best of science-fiction custom parsing
			#
			#p = lxml.html.HTMLParser()
			#t = lxml.html.parse(StringIO.StringIO(req.text), p)
			#o = t.xpath("//tr[@itemtype='http://schema.org/Book']")
			#for i in range(0,len(o)):
			#	book = o[i].xpath(".//a[@class='bookTitle']//span")
			#	if book:
			#		book_title = book[0].text.encode('utf-8')
			#	url = o[i].xpath(".//a[@class='bookTitle']")
			#	if url:
			#		book_url = url[0].get('href').encode('utf-8')
			#		book_id = re.sub('.*/(\d+).*', '\\1', book_url)
			#		req = session.get(GOOD + book_url, cookies=COOKIES)
			#		if req.status_code == 200:
			#			p2 = lxml.html.HTMLParser()
			#			t2 = lxml.html.parse(StringIO.StringIO(req.text), p2)
			#			o2 = t2.xpath("//div[@id='details']//div[@class='row']")
			#			book_year = ''
			#			if len(o2) == 2:
			#				book_year_match = re.findall('\d{4}', o2[1].text)
			#				if book_year_match:
			#					book_year = book_year_match[0]
			#			else: print(book_url)
			#	author = o[i].xpath(".//a[@class='authorName']//span")
			#	if author:
			#		book_author = author[0].text.encode('utf-8')
			#	data = o[i].xpath(".//span[@class='greyText smallText uitext']//span[@class='minirating']/text()")
			#	if data:
			#		book_avg = re.sub('(.*) avg.*', '\\1', data[0]).strip()
			#		book_ratings = re.sub(u'.*\u2014(.*)ratings$', '\\1', data[0]).replace(',','').strip()
			#	DATASET[book_id] = {
			#		'title':book_title.replace(';',' -'),
			#		'author':book_author,
			#		'url':book_url,
			#		'avg':book_avg,
			#		'ratings':book_ratings,
			#		'year':book_year
			#	}

		# hold on a second..
		time.sleep(1)

	print('\nDone.')

	# WRITE DATASET
	with open(OUTFILE, 'w') as f: json.dump(DATASET, f)

if args.plot:
	# LOAD DATASET
	DATASET={}
	try:
		with open(OUTFILE, 'r') as f: DATASET = json.load(f)
	except:
		print('Failed to load dataset.')
		sys.exit(1)

	# CLEANUP
	for book in DATASET.keys():
		for s in ('Shadowrun','Transmetropolitan','Blame','Battle Angel','Appleseed','Vol'):
			try:
				if re.match('.*' + s + '.*', DATASET[book]['title']):
					del DATASET[book]
			except: pass

	print('Generating plot .. (this may take a while)')

	# PLOT
	plt.subplots(figsize=(5,5))

	for book in DATASET:
		# COLOR BY YEAR
		if DATASET[book]['year'] == 0: COLOR = '#F38460' # red-orange
		if DATASET[book]['year'] < 1990: COLOR = '#008856' # green
		if DATASET[book]['year'] >= 1990 and int(book_year) < 2003: COLOR = '#F3C300' # yellow
		if DATASET[book]['year'] >= 2003 and int(book_year) < 2010: COLOR = '#A1CAF1' # blue
		if DATASET[book]['year'] >= 2010: COLOR = '#C2B280' # brown

		# SCATTER
		if DATASET[book]['year']:
			if DATASET[book]['year'] > args.year_min and DATASET[book]['year'] < args.year_max:
				plt.plot(
					DATASET[book]['ratings'],
					DATASET[book]['avg'],
					'o',
					markeredgewidth = 1.5,
					markeredgecolor = 'black',
					markerfacecolor = COLOR,
					markersize = 7		
				)

	# MEAN OF RATING AVERAGE
	plt.axhline(y=mean([DATASET[book]['avg'] for book in DATASET]), linewidth=1.5, color='r')
	# MEAN OF RATING COUNT
	plt.axvline(x=mean([DATASET[book]['ratings'] for book in DATASET]), linewidth=1.5, color='r')

	#  TUNE
	plt.title(PLOT_TITLE, fontsize=14)
	plt.xlabel('Number of ratings', fontsize=13)
	plt.ylabel('Average Rating', fontsize=13)
	plt.grid(True)
	plt.ticklabel_format(useOffset=True)
	plt.yticks([round(x * 0.1, 1) for x in range(38, 51)])# float range by Python..
	plt.xlim(right=args.cnt_max)
	plt.ylim(args.avg_min,args.avg_max)

	# CREATE IMAGE
	plt.savefig(filename=OUTPNG, format='png', dpi=120)
	plt.close()

if args.filter:
	# LOAD DATASET
	DATASET={}
	try:
		with open(OUTFILE, 'r') as f: DATASET = json.load(f)
	except:
		print('Failed to load dataset.')
		sys.exit(1)
	
	# CLEANUP
	for book in DATASET.keys():
		for s in ('Shadowrun','Transmetropolitan','Blame','Battle Angel','Appleseed','Vol'):
			try:
				if re.match('.*' + s + '.*', DATASET[book]['title']):
					del DATASET[book]
			except: pass
		
	# FILTER
	for book in DATASET:
		if DATASET[book]['year'] > args.year_min and DATASET[book]['year'] < args.year_max:
			if DATASET[book]['avg'] > args.avg_min and DATASET[book]['avg'] < args.avg_max:
				if DATASET[book]['ratings'] > args.cnt_min and DATASET[book]['ratings'] < args.cnt_max:
					print(
						str(DATASET[book]['avg']) + ' - ' +
						DATASET[book]['title'] + ' - ' +
						DATASET[book]['author']
					)

# EXIT
sys.exit(0)

